{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ad78c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4bd7a4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFNN:\n",
    "    def __init__(self, layer_sizes, \n",
    "                 activation_fn, activation_fn_deriv, \n",
    "                 output_activation_fn, output_activation_fn_deriv, \n",
    "                 loss_fn, loss_fn_deriv, task_type='classification'):\n",
    "        \n",
    "        self.num_layers = len(layer_sizes)\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.activation_fn = activation_fn\n",
    "        self.activation_fn_deriv = activation_fn_deriv\n",
    "        self.output_activation_fn = output_activation_fn\n",
    "        self.output_activation_fn_deriv = output_activation_fn_deriv\n",
    "        self.loss_fn = loss_fn\n",
    "        self.loss_fn_deriv = loss_fn_deriv\n",
    "        self.task_type = task_type\n",
    "        \n",
    "        # Initialize weights and biases (He initialization)\n",
    "        self.W = [np.random.randn(layer_sizes[i+1], layer_sizes[i]) * np.sqrt(2./layer_sizes[i]) \n",
    "                  for i in range(self.num_layers-1)]\n",
    "        self.b = [np.random.randn(layer_sizes[i+1], 1) for i in range(self.num_layers-1)]\n",
    "\n",
    "    def forward(self, x):\n",
    "        a = [x]\n",
    "        z = []\n",
    "        \n",
    "        # Hidden layers\n",
    "        for i in range(self.num_layers-2):\n",
    "            z_i = self.W[i].dot(a[i]) + self.b[i]\n",
    "            a_i = self.activation_fn(z_i)\n",
    "            z.append(z_i)\n",
    "            a.append(a_i)\n",
    "        \n",
    "        # Output layer\n",
    "        z_out = self.W[-1].dot(a[-1]) + self.b[-1]\n",
    "        a_out = self.output_activation_fn(z_out)\n",
    "        z.append(z_out)\n",
    "        a.append(a_out)\n",
    "        \n",
    "        return a_out, a, z\n",
    "    \n",
    "    def compute_loss(self, predictions, t):\n",
    "        # Ensure the target shape matches the predictions\n",
    "        if t.shape[0] != predictions.shape[0]:\n",
    "            t = t.T\n",
    "        return self.loss_fn(predictions, t)\n",
    "    \n",
    "    def _one_hot(self, inputs):\n",
    "        n_values = len(np.unique(inputs))\n",
    "        return np.eye(n_values)[:,inputs]\n",
    "    \n",
    "    def backward(self, x, t, a, z):\n",
    "        m = x.shape[1]\n",
    "        dL_dw = [np.zeros_like(w) for w in self.W]\n",
    "        dL_db = [np.zeros_like(b) for b in self.b]\n",
    "        \n",
    "        # Ensure the target shape matches the predictions\n",
    "        if t.shape[0] != a[-1].shape[0]:\n",
    "            t = t.T\n",
    "        \n",
    "        dL_da = self.loss_fn_deriv(a[-1], t)\n",
    "        dL_dz = dL_da * self.output_activation_fn_deriv(z[-1])\n",
    "        \n",
    "        dL_dw[-1] = dL_dz.dot(a[-2].T)\n",
    "        dL_db[-1] = np.sum(dL_dz, axis=1, keepdims=True)\n",
    "        \n",
    "        for i in range(self.num_layers-3, -1, -1):\n",
    "            dL_da = self.W[i+1].T.dot(dL_dz)\n",
    "            dL_dz = dL_da * self.activation_fn_deriv(z[i])\n",
    "            dL_dw[i] = dL_dz.dot(a[i].T)\n",
    "            dL_db[i] = np.sum(dL_dz, axis=1, keepdims=True)\n",
    "        \n",
    "        return dL_dw, dL_db\n",
    "    \n",
    "    def update_weights(self, dL_dw, dL_db, learning_rate):\n",
    "        self.W = [w - learning_rate * dw for w, dw in zip(self.W, dL_dw)]\n",
    "        self.b = [b - learning_rate * db for b, db in zip(self.b, dL_db)]\n",
    "        \n",
    "    def train(self, X, labels, learning_rate=0.01, epochs=1000, batch_size=None):\n",
    "        \"\"\"Trains the neural network using the given training data and labels.\"\"\"\n",
    "        m = X.shape[1]\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            if batch_size:  # If batch size is specified, use mini-batch gradient descent\n",
    "                num_batches = m // batch_size\n",
    "                for batch in range(num_batches):\n",
    "                    X_batch = X[:, batch*batch_size:(batch+1)*batch_size]\n",
    "                    labels_batch = labels[batch*batch_size:(batch+1)*batch_size]\n",
    "\n",
    "                    # Forward pass\n",
    "                    predictions, a_batch, z_batch = self.forward(X_batch)\n",
    "                    \n",
    "                    # Backward pass\n",
    "                    dL_dw, dL_db = self.backward(X_batch, labels_batch, a_batch, z_batch)\n",
    "\n",
    "                    # Update weights and biases\n",
    "                    self.update_weights(dL_dw, dL_db, learning_rate)\n",
    "            else:  # Otherwise, use batch gradient descent\n",
    "                # Forward pass\n",
    "                predictions, a_full, z_full = self.forward(X)\n",
    "                \n",
    "                # Backward pass\n",
    "                dL_dw, dL_db = self.backward(X, labels, a_full, z_full)\n",
    "\n",
    "                # Update weights and biases\n",
    "                self.update_weights(dL_dw, dL_db, learning_rate)\n",
    "\n",
    "            # Print loss at the end of each 100 epochs:\n",
    "            if epoch % 100 == 0:\n",
    "                # Ensure activations are for the entire dataset\n",
    "                predictions, _, _ = self.forward(X)\n",
    "                loss = self.compute_loss(predictions, labels)\n",
    "                print(f\"Epoch {epoch+1}/{epochs} - Loss: {np.mean(loss)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f9121a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU(inputs):\n",
    "    return np.maximum(inputs, 0)\n",
    "def deriv_ReLU(Z):\n",
    "    return Z>0\n",
    "\n",
    "def sigmoid(inputs, clip_value=200):\n",
    "    inputs = np.clip(inputs, -clip_value, clip_value)\n",
    "    return 1 / (1 + np.exp(-inputs))\n",
    "\n",
    "def deriv_sigmoid(inputs):\n",
    "    return(inputs*(1-inputs))\n",
    "\n",
    "def Linear(inputs):\n",
    "    return (1/10)*inputs\n",
    "\n",
    "def deriv_Linear(inputs):\n",
    "    return inputs*0 + (1/10)\n",
    "\n",
    "def clipped_ReLU(x, c=1):\n",
    "    return np.minimum(np.maximum(0, x), c)\n",
    "\n",
    "def deriv_clipped_ReLU(inputs):\n",
    "    return (inputs>0) & (inputs<1)\n",
    "\n",
    "def L2(outputs, targets):\n",
    "    return 0.5*sum((outputs - targets)**2)\n",
    "\n",
    "def deriv_L2(outputs, targets):\n",
    "    return outputs - targets\n",
    "\n",
    "def one_hot(inputs):\n",
    "    n_values = np.max(inputs) + 1\n",
    "    return np.eye(n_values)[:,inputs]\n",
    "\n",
    "def softmax(Z):\n",
    "    shiftZ = Z - np.max(Z, axis=0)\n",
    "    exps = np.exp(shiftZ)\n",
    "    return exps / np.sum(exps, axis=0)\n",
    "\n",
    "def dummy_deriv_softmax(Z):\n",
    "    return Z\n",
    "\n",
    "def categorical_cross_entropy(predictions, labels):\n",
    "    m = labels.shape[1]\n",
    "    return -np.sum(np.log(predictions) * labels) / m\n",
    "\n",
    "def deriv_cat_cross_entropy(predictions, labels):\n",
    "    return predictions - labels  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20e5fb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_sizes = [3,30,3]\n",
    "\n",
    "ffnn = FFNN(layer_sizes = layer_sizes, \n",
    "                activation_fn = ReLU, \n",
    "                activation_fn_deriv = deriv_ReLU, \n",
    "                output_activation_fn = Linear, \n",
    "                output_activation_fn_deriv = deriv_Linear, \n",
    "                loss_fn = L2, \n",
    "                loss_fn_deriv = deriv_L2,\n",
    "                task_type='regression')\n",
    "\n",
    "ffnn.train(X.T, Y, learning_rate=0.001, epochs = 3000, batch_size=2000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "345.352px",
    "left": "1410.98px",
    "right": "20px",
    "top": "69.9707px",
    "width": "379.482px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
