{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ad78c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "b6875182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.31597654]\n",
      " [0.27385159]\n",
      " [0.65352828]\n",
      " [0.98682435]\n",
      " [0.8852986 ]\n",
      " [0.09748676]\n",
      " [0.145739  ]\n",
      " [0.78769725]\n",
      " [0.83536539]\n",
      " [0.9619359 ]]\n",
      "[[-17.60223525]\n",
      " [-17.62563801]\n",
      " [-17.41470651]\n",
      " [-17.22954203]\n",
      " [-17.28594522]\n",
      " [-17.72361847]\n",
      " [-17.69681167]\n",
      " [-17.34016819]\n",
      " [-17.31368589]\n",
      " [-17.24336895]]\n"
     ]
    }
   ],
   "source": [
    "def generate_temp_data(n):\n",
    "    farenheit = np.random.rand(n).reshape(n,1)\n",
    "    celcius = (farenheit-32) * (5/9)\n",
    "    return farenheit, celcius\n",
    "\n",
    "X, Y = generate_temp_data(5000)\n",
    "print(X[0:10])\n",
    "print(Y[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "4bd7a4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFNN:\n",
    "    def __init__(self, layer_sizes, \n",
    "                 activation_fn, activation_fn_deriv, \n",
    "                 output_activation_fn, output_activation_fn_deriv, \n",
    "                 loss_fn, loss_fn_deriv, task_type='classification', lambda_val = 0):\n",
    "        \n",
    "        self.num_layers = len(layer_sizes)\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.activation_fn = activation_fn\n",
    "        self.activation_fn_deriv = activation_fn_deriv\n",
    "        self.output_activation_fn = output_activation_fn\n",
    "        self.output_activation_fn_deriv = output_activation_fn_deriv\n",
    "        self.loss_fn = loss_fn\n",
    "        self.loss_fn_deriv = loss_fn_deriv\n",
    "        self.task_type = task_type\n",
    "        self.lambda_val = lambda_val\n",
    "        \n",
    "        # Initialize weights and biases (He initialization)\n",
    "        self.W = [np.random.randn(layer_sizes[i+1], layer_sizes[i]) * np.sqrt(2./layer_sizes[i]) \n",
    "                  for i in range(self.num_layers-1)]\n",
    "        self.b = [np.random.randn(layer_sizes[i+1], 1)*0 for i in range(self.num_layers-1)]\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        a = [x]\n",
    "        z = []\n",
    "        \n",
    "        # Hidden layers\n",
    "        for i in range(self.num_layers-2):\n",
    "            z_i = self.W[i].dot(a[i]) + self.b[i]\n",
    "            a_i = self.activation_fn(z_i)\n",
    "            z.append(z_i)\n",
    "            a.append(a_i)\n",
    "        \n",
    "        # Output layer\n",
    "        z_out = self.W[-1].dot(a[-1]) + self.b[-1]\n",
    "        a_out = self.output_activation_fn(z_out)\n",
    "        z.append(z_out)\n",
    "        a.append(a_out)\n",
    "        \n",
    "        return a_out, a, z\n",
    "    \n",
    "    def compute_loss(self, predictions, t):\n",
    "        # Ensure the target shape matches the predictions\n",
    "        if t.shape[0] != predictions.shape[0]:\n",
    "            t = t.T\n",
    "\n",
    "        # Original loss without regularization\n",
    "        loss_without_reg = self.loss_fn(predictions, t)\n",
    "\n",
    "        # L1 regularization penalty\n",
    "        l1_penalty = sum([abs(w).sum() for w in self.W])\n",
    "\n",
    "        # Total loss with regularization\n",
    "        total_loss = loss_without_reg + self.lambda_val * l1_penalty\n",
    "\n",
    "        return total_loss\n",
    "    \n",
    "    def _one_hot(self, inputs):\n",
    "        n_values = len(np.unique(inputs))\n",
    "        return np.eye(n_values)[:,inputs]\n",
    "    \n",
    "    def backward(self, x, t, a, z):\n",
    "        m = x.shape[1]\n",
    "        dL_dw = [np.zeros_like(w) for w in self.W]\n",
    "        dL_db = [np.zeros_like(b) for b in self.b]\n",
    "        \n",
    "        # Ensure the target shape matches the predictions\n",
    "        if t.shape[0] != a[-1].shape[0]:\n",
    "            t = t.T\n",
    "        \n",
    "        dL_da = self.loss_fn_deriv(a[-1], t)\n",
    "        dL_dz = dL_da * self.output_activation_fn_deriv(z[-1])\n",
    "        \n",
    "        dL_dw[-1] = dL_dz.dot(a[-2].T)\n",
    "        dL_db[-1] = np.sum(dL_dz, axis=1, keepdims=True)\n",
    "        \n",
    "        for i in range(self.num_layers-3, -1, -1):\n",
    "            dL_da = self.W[i+1].T.dot(dL_dz)\n",
    "            dL_dz = dL_da * self.activation_fn_deriv(z[i])\n",
    "            dL_dw[i] = dL_dz.dot(a[i].T)\n",
    "            dL_db[i] = np.sum(dL_dz, axis=1, keepdims=True)\n",
    "        \n",
    "        return dL_dw, dL_db\n",
    "    \n",
    "    def update_weights(self, dL_dw, dL_db, learning_rate): #with L1 regularization\n",
    "        # Adjust the gradient for L1 regularization\n",
    "        dL_dw_regularized = [dw + self.lambda_val * np.sign(w) for w, dw in zip(self.W, dL_dw)]\n",
    "\n",
    "        # Update weights and biases using the adjusted gradient\n",
    "        self.W = [w - learning_rate * dw_reg for w, dw_reg in zip(self.W, dL_dw_regularized)]\n",
    "        self.b = [b - learning_rate * db for b, db in zip(self.b, dL_db)]\n",
    "        \n",
    "    def train(self, X, labels, learning_rate=0.01, epochs=1000, batch_size=None):\n",
    "        \"\"\"Trains the neural network using the given training data and labels.\"\"\"\n",
    "        m = X.shape[1]\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            if batch_size:  # If batch size is specified, use mini-batch gradient descent\n",
    "                num_batches = m // batch_size\n",
    "                for batch in range(num_batches):\n",
    "                    X_batch = X[:, batch*batch_size:(batch+1)*batch_size]\n",
    "                    labels_batch = labels[batch*batch_size:(batch+1)*batch_size]\n",
    "\n",
    "                    # Forward pass\n",
    "                    predictions, a_batch, z_batch = self.forward(X_batch)\n",
    "                    \n",
    "                    # Backward pass\n",
    "                    dL_dw, dL_db = self.backward(X_batch, labels_batch, a_batch, z_batch)\n",
    "\n",
    "                    # Update weights and biases\n",
    "                    self.update_weights(dL_dw, dL_db, learning_rate)\n",
    "            else:  # Otherwise, use batch gradient descent\n",
    "                # Forward pass\n",
    "                predictions, a_full, z_full = self.forward(X)\n",
    "                \n",
    "                # Backward pass\n",
    "                dL_dw, dL_db = self.backward(X, labels, a_full, z_full)\n",
    "\n",
    "                # Update weights and biases\n",
    "                self.update_weights(dL_dw, dL_db, learning_rate)\n",
    "\n",
    "            # Print loss at the end of each 100 epochs:\n",
    "            if epoch % 100 == 0:\n",
    "                # Ensure activations are for the entire dataset\n",
    "                predictions, _, _ = self.forward(X)\n",
    "                loss = self.compute_loss(predictions, labels)\n",
    "                print(f\"Epoch {epoch+1}/{epochs} - Loss: {np.mean(loss)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "1f9121a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Helper functions '''\n",
    "\n",
    "def ReLU(inputs):\n",
    "    return np.maximum(inputs, 0)\n",
    "def deriv_ReLU(Z):\n",
    "    return Z>0\n",
    "\n",
    "def Linear(inputs):\n",
    "    return inputs\n",
    "\n",
    "def deriv_Linear(inputs):\n",
    "    return inputs*0 + 1\n",
    "\n",
    "def L2(outputs, targets):\n",
    "    return 0.5*sum((outputs.flatten() - targets.flatten())**2)\n",
    "\n",
    "def deriv_L2(outputs, targets):\n",
    "    return outputs - targets\n",
    "\n",
    "def one_hot(inputs):\n",
    "    n_values = np.max(inputs) + 1\n",
    "    return np.eye(n_values)[:,inputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "20e5fb5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20000 - Loss: 693194.3364847661\n",
      "Epoch 101/20000 - Loss: 10617.22514558033\n",
      "Epoch 201/20000 - Loss: 3668.0836079598935\n",
      "Epoch 301/20000 - Loss: 1166.7492341661632\n",
      "Epoch 401/20000 - Loss: 343.3090315443343\n",
      "Epoch 501/20000 - Loss: 95.74325269306856\n",
      "Epoch 601/20000 - Loss: 25.86441559559123\n",
      "Epoch 701/20000 - Loss: 6.868780831522536\n",
      "Epoch 801/20000 - Loss: 1.8113914478503192\n",
      "Epoch 901/20000 - Loss: 0.47971768537575515\n",
      "Epoch 1001/20000 - Loss: 0.13108179614017662\n",
      "Epoch 1101/20000 - Loss: 0.04007646058116789\n",
      "Epoch 1201/20000 - Loss: 0.016356096892773724\n",
      "Epoch 1301/20000 - Loss: 0.010177645171066271\n",
      "Epoch 1401/20000 - Loss: 0.008568674287240279\n",
      "Epoch 1501/20000 - Loss: 0.008149599250340568\n",
      "Epoch 1601/20000 - Loss: 0.008040376737096877\n",
      "Epoch 1701/20000 - Loss: 0.008011870048017919\n",
      "Epoch 1801/20000 - Loss: 0.008004408252624627\n",
      "Epoch 1901/20000 - Loss: 0.008002443432691552\n",
      "Epoch 2001/20000 - Loss: 0.008001919580181073\n",
      "Epoch 2101/20000 - Loss: 0.008001776083232534\n",
      "Epoch 2201/20000 - Loss: 0.008001734313606979\n",
      "Epoch 2301/20000 - Loss: 0.008001720419855689\n",
      "Epoch 2401/20000 - Loss: 0.00800171449662676\n",
      "Epoch 2501/20000 - Loss: 0.008001711013134485\n",
      "Epoch 2601/20000 - Loss: 0.00800170835104877\n",
      "Epoch 2701/20000 - Loss: 0.008001705997868886\n",
      "Epoch 2801/20000 - Loss: 0.008001703773615237\n",
      "Epoch 2901/20000 - Loss: 0.008001701607673224\n",
      "Epoch 3001/20000 - Loss: 0.008001699469536633\n",
      "Epoch 3101/20000 - Loss: 0.008001697345079155\n",
      "Epoch 3201/20000 - Loss: 0.008001695227468224\n",
      "Epoch 3301/20000 - Loss: 0.008001693113315672\n",
      "Epoch 3401/20000 - Loss: 0.00800169100091846\n",
      "Epoch 3501/20000 - Loss: 0.008001688889414445\n",
      "Epoch 3601/20000 - Loss: 0.008001686778365547\n",
      "Epoch 3701/20000 - Loss: 0.008001684667548728\n",
      "Epoch 3801/20000 - Loss: 0.008001682556850354\n",
      "Epoch 3901/20000 - Loss: 0.008001680446212488\n",
      "Epoch 4001/20000 - Loss: 0.008001678335605562\n",
      "Epoch 4101/20000 - Loss: 0.008001676225014518\n",
      "Epoch 4201/20000 - Loss: 0.008001674114431688\n",
      "Epoch 4301/20000 - Loss: 0.008001672003853027\n",
      "Epoch 4401/20000 - Loss: 0.008001669893276725\n",
      "Epoch 4501/20000 - Loss: 0.008001667782701649\n",
      "Epoch 4601/20000 - Loss: 0.008001665672127263\n",
      "Epoch 4701/20000 - Loss: 0.008001663561553346\n",
      "Epoch 4801/20000 - Loss: 0.008001661450979717\n",
      "Epoch 4901/20000 - Loss: 0.00800165934040642\n",
      "Epoch 5001/20000 - Loss: 0.008001657229833353\n",
      "Epoch 5101/20000 - Loss: 0.00800165511926049\n",
      "Epoch 5201/20000 - Loss: 0.008001653008687708\n",
      "Epoch 5301/20000 - Loss: 0.00800165089811513\n",
      "Epoch 5401/20000 - Loss: 0.008001648787542837\n",
      "Epoch 5501/20000 - Loss: 0.008001646676970715\n",
      "Epoch 5601/20000 - Loss: 0.008001644566398732\n",
      "Epoch 5701/20000 - Loss: 0.008001642455827\n",
      "Epoch 5801/20000 - Loss: 0.008001640345255396\n",
      "Epoch 5901/20000 - Loss: 0.008001638234684047\n",
      "Epoch 6001/20000 - Loss: 0.008001636124112857\n",
      "Epoch 6101/20000 - Loss: 0.008001634013541857\n",
      "Epoch 6201/20000 - Loss: 0.008001631902971061\n",
      "Epoch 6301/20000 - Loss: 0.008001629792400376\n",
      "Epoch 6401/20000 - Loss: 0.008001627681829887\n",
      "Epoch 6501/20000 - Loss: 0.00800162557125957\n",
      "Epoch 6601/20000 - Loss: 0.008001623460689465\n",
      "Epoch 6701/20000 - Loss: 0.008001621350119522\n",
      "Epoch 6801/20000 - Loss: 0.008001619239549684\n",
      "Epoch 6901/20000 - Loss: 0.008001617128980043\n",
      "Epoch 7001/20000 - Loss: 0.00800161501841062\n",
      "Epoch 7101/20000 - Loss: 0.00800161290784143\n",
      "Epoch 7201/20000 - Loss: 0.008001610797272362\n",
      "Epoch 7301/20000 - Loss: 0.008001608686703523\n",
      "Epoch 7401/20000 - Loss: 0.00800160657613489\n",
      "Epoch 7501/20000 - Loss: 0.008001604465566484\n",
      "Epoch 7601/20000 - Loss: 0.008001602354998333\n",
      "Epoch 7701/20000 - Loss: 0.008001600244430283\n",
      "Epoch 7801/20000 - Loss: 0.008001598133862373\n",
      "Epoch 7901/20000 - Loss: 0.008001596023294684\n",
      "Epoch 8001/20000 - Loss: 0.00800159391272719\n",
      "Epoch 8101/20000 - Loss: 0.008001591802159877\n",
      "Epoch 8201/20000 - Loss: 0.008001589691592738\n",
      "Epoch 8301/20000 - Loss: 0.00800158758102579\n",
      "Epoch 8401/20000 - Loss: 0.008001585470459042\n",
      "Epoch 8501/20000 - Loss: 0.008001583359892428\n",
      "Epoch 8601/20000 - Loss: 0.008001581249326144\n",
      "Epoch 8701/20000 - Loss: 0.00800157913875998\n",
      "Epoch 8801/20000 - Loss: 0.008001577028193946\n",
      "Epoch 8901/20000 - Loss: 0.008001574917628063\n",
      "Epoch 9001/20000 - Loss: 0.008001572807062391\n",
      "Epoch 9101/20000 - Loss: 0.008001570696496934\n",
      "Epoch 9201/20000 - Loss: 0.008001568585931602\n",
      "Epoch 9301/20000 - Loss: 0.008001566475366541\n",
      "Epoch 9401/20000 - Loss: 0.008001564364801608\n",
      "Epoch 9501/20000 - Loss: 0.008001562254236807\n",
      "Epoch 9601/20000 - Loss: 0.00800156014367233\n",
      "Epoch 9701/20000 - Loss: 0.008001558033107986\n",
      "Epoch 9801/20000 - Loss: 0.00800155592254375\n",
      "Epoch 9901/20000 - Loss: 0.008001553811979673\n",
      "Epoch 10001/20000 - Loss: 0.008001551701415895\n",
      "Epoch 10101/20000 - Loss: 0.008001549590852258\n",
      "Epoch 10201/20000 - Loss: 0.008001547480288725\n",
      "Epoch 10301/20000 - Loss: 0.008001545369725478\n",
      "Epoch 10401/20000 - Loss: 0.008001543259162474\n",
      "Epoch 10501/20000 - Loss: 0.008001541148599637\n",
      "Epoch 10601/20000 - Loss: 0.008001539038036855\n",
      "Epoch 10701/20000 - Loss: 0.008001536927474259\n",
      "Epoch 10801/20000 - Loss: 0.00800153481691193\n",
      "Epoch 10901/20000 - Loss: 0.008001532706349809\n",
      "Epoch 11001/20000 - Loss: 0.008001530595787862\n",
      "Epoch 11101/20000 - Loss: 0.008001528485226015\n",
      "Epoch 11201/20000 - Loss: 0.008001526374664352\n",
      "Epoch 11301/20000 - Loss: 0.00800152426410302\n",
      "Epoch 11401/20000 - Loss: 0.008001522153541824\n",
      "Epoch 11501/20000 - Loss: 0.008001520042980742\n",
      "Epoch 11601/20000 - Loss: 0.008001517932419865\n",
      "Epoch 11701/20000 - Loss: 0.0080015158218592\n",
      "Epoch 11801/20000 - Loss: 0.008001513711298635\n",
      "Epoch 11901/20000 - Loss: 0.008001511600738325\n",
      "Epoch 12001/20000 - Loss: 0.008001509490178233\n",
      "Epoch 12101/20000 - Loss: 0.008001507379618278\n",
      "Epoch 12201/20000 - Loss: 0.008001505269058464\n",
      "Epoch 12301/20000 - Loss: 0.008001503158498915\n",
      "Epoch 12401/20000 - Loss: 0.00800150104793956\n",
      "Epoch 12501/20000 - Loss: 0.008001498937380348\n",
      "Epoch 12601/20000 - Loss: 0.00800149682682133\n",
      "Epoch 12701/20000 - Loss: 0.008001494716262514\n",
      "Epoch 12801/20000 - Loss: 0.008001492605703825\n",
      "Epoch 12901/20000 - Loss: 0.00800149049514543\n",
      "Epoch 13001/20000 - Loss: 0.008001488384587231\n",
      "Epoch 13101/20000 - Loss: 0.008001486274029183\n",
      "Epoch 13201/20000 - Loss: 0.008001484163471313\n",
      "Epoch 13301/20000 - Loss: 0.008001482052913608\n",
      "Epoch 13401/20000 - Loss: 0.008001479942356124\n",
      "Epoch 13501/20000 - Loss: 0.008001477831798833\n",
      "Epoch 13601/20000 - Loss: 0.008001475721241583\n",
      "Epoch 13701/20000 - Loss: 0.008001473610684604\n",
      "Epoch 13801/20000 - Loss: 0.00800147150012778\n",
      "Epoch 13901/20000 - Loss: 0.008001469389571102\n",
      "Epoch 14001/20000 - Loss: 0.008001467279014681\n",
      "Epoch 14101/20000 - Loss: 0.008001465168458366\n",
      "Epoch 14201/20000 - Loss: 0.008001463057902197\n",
      "Epoch 14301/20000 - Loss: 0.008001460947346332\n",
      "Epoch 14401/20000 - Loss: 0.008001458836790659\n",
      "Epoch 14501/20000 - Loss: 0.008001456726235028\n",
      "Epoch 14601/20000 - Loss: 0.008001454615679688\n",
      "Epoch 14701/20000 - Loss: 0.008001452505124606\n",
      "Epoch 14801/20000 - Loss: 0.008001450394569731\n",
      "Epoch 14901/20000 - Loss: 0.008001448284014959\n",
      "Epoch 15001/20000 - Loss: 0.008001446173460304\n",
      "Epoch 15101/20000 - Loss: 0.008001444062905845\n",
      "Epoch 15201/20000 - Loss: 0.00800144195235159\n",
      "Epoch 15301/20000 - Loss: 0.008001439841797646\n",
      "Epoch 15401/20000 - Loss: 0.008001437731243824\n",
      "Epoch 15501/20000 - Loss: 0.008001435620690152\n",
      "Epoch 15601/20000 - Loss: 0.008001433510136605\n",
      "Epoch 15701/20000 - Loss: 0.008001431399583314\n",
      "Epoch 15801/20000 - Loss: 0.008001429289030344\n",
      "Epoch 15901/20000 - Loss: 0.008001427178477473\n",
      "Epoch 16001/20000 - Loss: 0.008001425067924698\n",
      "Epoch 16101/20000 - Loss: 0.008001422957372026\n",
      "Epoch 16201/20000 - Loss: 0.008001420846819672\n",
      "Epoch 16301/20000 - Loss: 0.008001418736267599\n",
      "Epoch 16401/20000 - Loss: 0.008001416625715725\n",
      "Epoch 16501/20000 - Loss: 0.008001414515163827\n",
      "Epoch 16601/20000 - Loss: 0.008001412404612097\n",
      "Epoch 16701/20000 - Loss: 0.008001410294060726\n",
      "Epoch 16801/20000 - Loss: 0.0080014081835096\n",
      "Epoch 16901/20000 - Loss: 0.008001406072958538\n",
      "Epoch 17001/20000 - Loss: 0.008001403962407583\n",
      "Epoch 17101/20000 - Loss: 0.008001401851856813\n",
      "Epoch 17201/20000 - Loss: 0.00800139974130627\n",
      "Epoch 17301/20000 - Loss: 0.00800139763075604\n",
      "Epoch 17401/20000 - Loss: 0.008001395520205914\n",
      "Epoch 17501/20000 - Loss: 0.008001393409655874\n",
      "Epoch 17601/20000 - Loss: 0.00800139129910599\n",
      "Epoch 17701/20000 - Loss: 0.008001389188556328\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17801/20000 - Loss: 0.008001387078006949\n",
      "Epoch 17901/20000 - Loss: 0.008001384967457663\n",
      "Epoch 18001/20000 - Loss: 0.008001382856908514\n",
      "Epoch 18101/20000 - Loss: 0.008001380746359654\n",
      "Epoch 18201/20000 - Loss: 0.008001378635810899\n",
      "Epoch 18301/20000 - Loss: 0.008001376525262368\n",
      "Epoch 18401/20000 - Loss: 0.008001374414714104\n",
      "Epoch 18501/20000 - Loss: 0.008001372304165994\n",
      "Epoch 18601/20000 - Loss: 0.00800137019361801\n",
      "Epoch 18701/20000 - Loss: 0.008001368083070236\n",
      "Epoch 18801/20000 - Loss: 0.008001365972522741\n",
      "Epoch 18901/20000 - Loss: 0.008001363861975388\n",
      "Epoch 19001/20000 - Loss: 0.008001361751428187\n",
      "Epoch 19101/20000 - Loss: 0.008001359640881178\n",
      "Epoch 19201/20000 - Loss: 0.008001357530334442\n",
      "Epoch 19301/20000 - Loss: 0.008001355419787826\n",
      "Epoch 19401/20000 - Loss: 0.00800135330924131\n",
      "Epoch 19501/20000 - Loss: 0.008001351198694991\n",
      "Epoch 19601/20000 - Loss: 0.008001349088148915\n",
      "Epoch 19701/20000 - Loss: 0.00800134697760296\n",
      "Epoch 19801/20000 - Loss: 0.008001344867057262\n",
      "Epoch 19901/20000 - Loss: 0.00800134275651165\n"
     ]
    }
   ],
   "source": [
    "layer_sizes = [1,2,1]\n",
    "\n",
    "ffnn = FFNN(layer_sizes = layer_sizes, \n",
    "                activation_fn = Linear, \n",
    "                activation_fn_deriv = deriv_Linear, \n",
    "                output_activation_fn = Linear, \n",
    "                output_activation_fn_deriv = deriv_Linear, \n",
    "                loss_fn = L2, \n",
    "                loss_fn_deriv = deriv_L2,\n",
    "                task_type='regression',\n",
    "                lambda_val=0.001)\n",
    "\n",
    "ffnn.train(X.T, Y, learning_rate=0.000001, epochs = 8000, batch_size=700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "ab843a8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1486846122167238e-09\n"
     ]
    }
   ],
   "source": [
    "'''check the conversion capabilities'''\n",
    "cel_real = (X.T-32) * (5/9)\n",
    "cel_pred,_,_ = ffnn.forward(X.T)\n",
    "\n",
    "print(L2(cel_pred.T, cel_real.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "d0d12254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[1.24669922],\n",
      "       [1.05507221]]), array([[-2.37117486,  3.32839323]])]\n",
      "[array([[ 2.75109627],\n",
      "       [-2.90859269]]), array([[-1.57350588]])]\n"
     ]
    }
   ],
   "source": [
    "''' print weights and biases to begin grokking the network '''\n",
    "print(ffnn.W)\n",
    "print(ffnn.b)\n",
    "w11, w12, w21, w22 = (ffnn.W[0][0][0], ffnn.W[0][1][0], ffnn.W[1][0][0], ffnn.W[1][0][1])\n",
    "bh1, bh2, bo = (ffnn.b[0][0][0], ffnn.b[0][1][0], ffnn.b[1][0][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "fd919e05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-20.555543161614352\n",
      "-17.777776424618622\n",
      "-15.000009687622898\n",
      "-6.930784598901951e-05\n"
     ]
    }
   ],
   "source": [
    "def learned_conversion(f):\n",
    "    h1 = w11 * f + bh1\n",
    "    h2 = w12 * f + bh2\n",
    "    o = w21 * h1 + w22 * h2 + bo\n",
    "    return o\n",
    "\n",
    "temps = [-5, 0, 5, 32]\n",
    "for t in temps:\n",
    "    print(learned_conversion(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "a5b16cc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learned slope =  0.5555533473991447\n",
      "learned intercept =  -17.777776424618622\n",
      "actual slope =  0.5555555555555556\n",
      "actual intercept =  -17.77777777777778\n"
     ]
    }
   ],
   "source": [
    "'''learned_conversion is equivalent to the following:\n",
    "(w21*w11 + w22*w12)*f + (w21*bh1 + w22*bh2 + bo) '''\n",
    "\n",
    "print(\"learned slope = \", (w21*w11 + w22*w12))\n",
    "print(\"learned intercept = \", (w21*bh1 + w22*bh2 + bo))\n",
    "\n",
    "'''actual formula is (f - 32) * 5/9 \n",
    "aka, (5/9)*f - 32*(5/9)'''\n",
    "\n",
    "print(\"actual slope = \", 5/9)\n",
    "print(\"actual intercept = \", (5/9)*-32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "a31ff0ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.20714478025367813, -0.10560242967165585)\n"
     ]
    }
   ],
   "source": [
    "''' as a study of L1 regularization, attenpt to use a single network to \n",
    "    differentiate between different equations '''\n",
    "\n",
    "def expr1(x, n1, n2, n3, n4):\n",
    "    return n1*x + n2, (n1, n2)\n",
    "\n",
    "def expr2(x, n1, n2, n3, n4):\n",
    "    return n1*x**2 + n2*x + n3, (n1, n2, n3)\n",
    "\n",
    "def expr3(x, n1, n2, n3, n4):\n",
    "    return n1*np.sin(x) + n2*x**2, (n1, n2)\n",
    "\n",
    "def expr4(x, n1, n2, n3, n4):\n",
    "    return n1*np.sin(x) + n2*x**2 + n3*x + n4, (n1, n2, n3, n4)\n",
    "\n",
    "def generate_data(n, expression):\n",
    "    X = np.random.uniform(low=-1.5, high=1.5, size = n)\n",
    "    n1, n2, n3, n4 = np.random.randn(4)\n",
    "    Y, params = expression(X, n1, n2, n3, n4)\n",
    "    return X, Y, params\n",
    "\n",
    "X, Y, params = generate_data(5000, expr3)\n",
    "\n",
    "print(params)\n",
    "\n",
    "square = X**2\n",
    "sin = np.sin(X)\n",
    "\n",
    "\n",
    "'''add in extra features to capture squares and sins'''\n",
    "X = np.c_[X, square, sin]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "e21bed3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12000 - Loss: 487.0904087675701\n",
      "Epoch 101/12000 - Loss: 0.7825122745583735\n",
      "Epoch 201/12000 - Loss: 0.31476867191643954\n",
      "Epoch 301/12000 - Loss: 0.2770520559691102\n",
      "Epoch 401/12000 - Loss: 0.2725081501388098\n",
      "Epoch 501/12000 - Loss: 0.2705915149248239\n",
      "Epoch 601/12000 - Loss: 0.26883076088705266\n",
      "Epoch 701/12000 - Loss: 0.26711613918586025\n",
      "Epoch 801/12000 - Loss: 0.26547573043488837\n",
      "Epoch 901/12000 - Loss: 0.26392239072971985\n",
      "Epoch 1001/12000 - Loss: 0.26245860745689936\n",
      "Epoch 1101/12000 - Loss: 0.2610830322065726\n",
      "Epoch 1201/12000 - Loss: 0.2597929616752977\n",
      "Epoch 1301/12000 - Loss: 0.2584540009668695\n",
      "Epoch 1401/12000 - Loss: 0.2577559558890432\n",
      "Epoch 1501/12000 - Loss: 0.2570022425594326\n",
      "Epoch 1601/12000 - Loss: 0.25618406293340895\n",
      "Epoch 1701/12000 - Loss: 0.25535422807902886\n",
      "Epoch 1801/12000 - Loss: 0.25454134597188904\n",
      "Epoch 1901/12000 - Loss: 0.2537392459579349\n",
      "Epoch 2001/12000 - Loss: 0.2529478827183936\n",
      "Epoch 2101/12000 - Loss: 0.25218972064411627\n",
      "Epoch 2201/12000 - Loss: 0.25142898166064936\n",
      "Epoch 2301/12000 - Loss: 0.250701607510457\n",
      "Epoch 2401/12000 - Loss: 0.24998477558245544\n",
      "Epoch 2501/12000 - Loss: 0.24927748101531333\n",
      "Epoch 2601/12000 - Loss: 0.24860413322927916\n",
      "Epoch 2701/12000 - Loss: 0.24792600164800596\n",
      "Epoch 2801/12000 - Loss: 0.24728223637716223\n",
      "Epoch 2901/12000 - Loss: 0.24664687278556768\n",
      "Epoch 3001/12000 - Loss: 0.24603268360968536\n",
      "Epoch 3101/12000 - Loss: 0.245427806422643\n",
      "Epoch 3201/12000 - Loss: 0.24484370123049348\n",
      "Epoch 3301/12000 - Loss: 0.2442675617800042\n",
      "Epoch 3401/12000 - Loss: 0.24371307541559917\n",
      "Epoch 3501/12000 - Loss: 0.24316616298012453\n",
      "Epoch 3601/12000 - Loss: 0.2426417816741181\n",
      "Epoch 3701/12000 - Loss: 0.24212532582032387\n",
      "Epoch 3801/12000 - Loss: 0.24163068339321117\n",
      "Epoch 3901/12000 - Loss: 0.2411436726042422\n",
      "Epoch 4001/12000 - Loss: 0.24066484037861205\n",
      "Epoch 4101/12000 - Loss: 0.24020980799957237\n",
      "Epoch 4201/12000 - Loss: 0.2397641520801732\n",
      "Epoch 4301/12000 - Loss: 0.23932395735990397\n",
      "Epoch 4401/12000 - Loss: 0.23891056072555303\n",
      "Epoch 4501/12000 - Loss: 0.23850556276557072\n",
      "Epoch 4601/12000 - Loss: 0.23811035107884201\n",
      "Epoch 4701/12000 - Loss: 0.23772429983372032\n",
      "Epoch 4801/12000 - Loss: 0.23734749562818636\n",
      "Epoch 4901/12000 - Loss: 0.2369809891550437\n",
      "Epoch 5001/12000 - Loss: 0.2366255788312364\n",
      "Epoch 5101/12000 - Loss: 0.2362808642516325\n",
      "Epoch 5201/12000 - Loss: 0.23594751787057916\n",
      "Epoch 5301/12000 - Loss: 0.23562633114651085\n",
      "Epoch 5401/12000 - Loss: 0.2353182378160682\n",
      "Epoch 5501/12000 - Loss: 0.23502336826916123\n",
      "Epoch 5601/12000 - Loss: 0.23472252694885068\n",
      "Epoch 5701/12000 - Loss: 0.23443652092211506\n",
      "Epoch 5801/12000 - Loss: 0.2341654707511852\n",
      "Epoch 5901/12000 - Loss: 0.23391010781187527\n",
      "Epoch 6001/12000 - Loss: 0.2336522674360208\n",
      "Epoch 6101/12000 - Loss: 0.23339013346639534\n",
      "Epoch 6201/12000 - Loss: 0.23314571868477124\n",
      "Epoch 6301/12000 - Loss: 0.23292113207693868\n",
      "Epoch 6401/12000 - Loss: 0.2326945044751972\n",
      "Epoch 6501/12000 - Loss: 0.23246686850431356\n",
      "Epoch 6601/12000 - Loss: 0.23226056055388702\n",
      "Epoch 6701/12000 - Loss: 0.232054682326052\n",
      "Epoch 6801/12000 - Loss: 0.2318499474291721\n",
      "Epoch 6901/12000 - Loss: 0.23164593570475678\n",
      "Epoch 7001/12000 - Loss: 0.2314676217083454\n",
      "Epoch 7101/12000 - Loss: 0.23129136126477623\n",
      "Epoch 7201/12000 - Loss: 0.23111904483683457\n",
      "Epoch 7301/12000 - Loss: 0.23095073725972043\n",
      "Epoch 7401/12000 - Loss: 0.23076498401273768\n",
      "Epoch 7501/12000 - Loss: 0.23060909032657592\n",
      "Epoch 7601/12000 - Loss: 0.2304594207006294\n",
      "Epoch 7701/12000 - Loss: 0.23031644424228345\n",
      "Epoch 7801/12000 - Loss: 0.23017941859455357\n",
      "Epoch 7901/12000 - Loss: 0.23003340650499765\n",
      "Epoch 8001/12000 - Loss: 0.22989307503417789\n",
      "Epoch 8101/12000 - Loss: 0.22976228006528582\n",
      "Epoch 8201/12000 - Loss: 0.22964076949090695\n",
      "Epoch 8301/12000 - Loss: 0.22951469264662594\n",
      "Epoch 8401/12000 - Loss: 0.22939361937176286\n",
      "Epoch 8501/12000 - Loss: 0.22927651429482696\n",
      "Epoch 8601/12000 - Loss: 0.22916313400832616\n",
      "Epoch 8701/12000 - Loss: 0.22905333509037043\n",
      "Epoch 8801/12000 - Loss: 0.22894699413617411\n",
      "Epoch 8901/12000 - Loss: 0.22884399996791577\n",
      "Epoch 9001/12000 - Loss: 0.2287442518099341\n",
      "Epoch 9101/12000 - Loss: 0.2286476580485955\n",
      "Epoch 9201/12000 - Loss: 0.22855413513846035\n",
      "Epoch 9301/12000 - Loss: 0.2284636066132087\n",
      "Epoch 9401/12000 - Loss: 0.22837600219153856\n",
      "Epoch 9501/12000 - Loss: 0.22829125697098315\n",
      "Epoch 9601/12000 - Loss: 0.2282093107030797\n",
      "Epoch 9701/12000 - Loss: 0.228130107143651\n",
      "Epoch 9801/12000 - Loss: 0.2280535934722792\n",
      "Epoch 9901/12000 - Loss: 0.22797971977537151\n",
      "Epoch 10001/12000 - Loss: 0.2279084385875426\n",
      "Epoch 10101/12000 - Loss: 0.2278397044863487\n",
      "Epoch 10201/12000 - Loss: 0.22777347373573095\n",
      "Epoch 10301/12000 - Loss: 0.2277097039738208\n",
      "Epoch 10401/12000 - Loss: 0.2276483539410594\n",
      "Epoch 10501/12000 - Loss: 0.22758938324485672\n",
      "Epoch 10601/12000 - Loss: 0.22753275215730853\n",
      "Epoch 10701/12000 - Loss: 0.22747842144271502\n",
      "Epoch 10801/12000 - Loss: 0.22742635221192595\n",
      "Epoch 10901/12000 - Loss: 0.2273765058007377\n",
      "Epoch 11001/12000 - Loss: 0.22732884366981598\n",
      "Epoch 11101/12000 - Loss: 0.2272833273238011\n",
      "Epoch 11201/12000 - Loss: 0.22723991824745746\n",
      "Epoch 11301/12000 - Loss: 0.22719857785691203\n",
      "Epoch 11401/12000 - Loss: 0.2271592674641916\n",
      "Epoch 11501/12000 - Loss: 0.22712194825343202\n",
      "Epoch 11601/12000 - Loss: 0.22708658126726045\n",
      "Epoch 11701/12000 - Loss: 0.22705312740203146\n",
      "Epoch 11801/12000 - Loss: 0.22702154741066174\n",
      "Epoch 11901/12000 - Loss: 0.22699180191199242\n"
     ]
    }
   ],
   "source": [
    "'''train model'''\n",
    "layer_sizes = [3, 1, 1]\n",
    "ffnn2 = FFNN(layer_sizes = layer_sizes, \n",
    "                activation_fn = Linear, \n",
    "                activation_fn_deriv = deriv_Linear, \n",
    "                output_activation_fn = Linear, \n",
    "                output_activation_fn_deriv = deriv_Linear, \n",
    "                loss_fn = L2, \n",
    "                loss_fn_deriv = deriv_L2,\n",
    "                task_type='regression',\n",
    "                lambda_val=0.2)\n",
    "ffnn2.train(X.T, Y, learning_rate=0.00005, epochs = 12000, batch_size=700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "7b008f5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0032652086556610653\n"
     ]
    }
   ],
   "source": [
    "'''check conversion capabilities (expr 1) '''\n",
    "args = (*params, 0, 0, 0)[:4]\n",
    "real, _ = expr3(X[:,0], *args)\n",
    "\n",
    "pred,_,_ = ffnn2.forward(X.T)\n",
    "\n",
    "print(L2(pred, real))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "32cb4a1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[-0.00646479,  0.20738773, -0.40131432]]), array([[-0.50332773]])]\n",
      "[array([[-0.05760433]]), array([[-0.02989564]])]\n"
     ]
    }
   ],
   "source": [
    "print(ffnn2.W)\n",
    "print(ffnn2.b)\n",
    "# threshold = 1e-4  # for example\n",
    "\n",
    "# for idx, w in enumerate(ffnn2.W):\n",
    "#     ffnn2.W[idx] = np.where(abs(w) < threshold, 0, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "9e3f1a9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(-0.1463186304698898, -0.7180350821061807, 0.8901555607951912)\n",
      "0.35199804019794356 -1.389567191087169\n"
     ]
    }
   ],
   "source": [
    "learned_param1 = ffnn2.W[0][0][0]*ffnn2.W[1][0][0]\n",
    "learned_param2 = ffnn2.b[0][0][0]*ffnn2.W[1][0][0] + ffnn2.b[1][0][0]\n",
    "\n",
    "print(params)\n",
    "print(learned_param1, learned_param2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "85ef3505",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(n, expression):\n",
    "    X = np.random.uniform(low=0.0, high=1.5, size = n)\n",
    "    n1, n2, n3, n4 = np.random.randn(4)\n",
    "    Y, params = expression(X, n1, n2, n3, n4)\n",
    "    return X, Y, params\n",
    "X, Y, params = generate_data(5000, expr2)\n",
    "square = X**2\n",
    "sin = np.sin(X)\n",
    "\n",
    "'''add in extra features to capture squares and sins'''\n",
    "X = np.c_[X, square, sin]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "345.352px",
    "left": "1410.98px",
    "right": "20px",
    "top": "69.9707px",
    "width": "379.482px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
