{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1ad78c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "b6875182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.4607624 ]\n",
      " [0.10815563]\n",
      " [0.5025832 ]\n",
      " [0.25980607]\n",
      " [0.82426736]\n",
      " [0.21169986]\n",
      " [0.25958015]\n",
      " [0.90772114]\n",
      " [0.98437963]\n",
      " [0.31629536]]\n",
      "[[-17.52179867]\n",
      " [-17.71769132]\n",
      " [-17.49856489]\n",
      " [-17.63344107]\n",
      " [-17.31985147]\n",
      " [-17.66016675]\n",
      " [-17.63356658]\n",
      " [-17.27348826]\n",
      " [-17.23090021]\n",
      " [-17.60205813]]\n"
     ]
    }
   ],
   "source": [
    "def generate_temp_data(n):\n",
    "    farenheit = np.random.rand(n).reshape(n,1)\n",
    "    celcius = (farenheit-32) * (5/9)\n",
    "    return farenheit, celcius\n",
    "\n",
    "X, Y = generate_temp_data(5000)\n",
    "print(X[0:10])\n",
    "print(Y[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "398f899e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_class_data(n):\n",
    "    dataset = []\n",
    "    labels = []\n",
    "    X = np.random.uniform(0,1,(n,2))\n",
    "    for point in X:\n",
    "        if point[1] < 0.5 and point[1] < (-1/0.5)*point[0] + 1:\n",
    "            labels.append(0)\n",
    "        else:\n",
    "            labels.append(1)\n",
    "            \n",
    "    return X, np.array(labels).astype('int')\n",
    "\n",
    "#X, labels = generate_class_data(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4bd7a4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFNN:\n",
    "    def __init__(self, layer_sizes, \n",
    "                 activation_fn, activation_fn_deriv, \n",
    "                 output_activation_fn, output_activation_fn_deriv, \n",
    "                 loss_fn, loss_fn_deriv, task_type='classification', lambda_val = 0):\n",
    "        \n",
    "        self.num_layers = len(layer_sizes)\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.activation_fn = activation_fn\n",
    "        self.activation_fn_deriv = activation_fn_deriv\n",
    "        self.output_activation_fn = output_activation_fn\n",
    "        self.output_activation_fn_deriv = output_activation_fn_deriv\n",
    "        self.loss_fn = loss_fn\n",
    "        self.loss_fn_deriv = loss_fn_deriv\n",
    "        self.task_type = task_type\n",
    "        self.lambda_val = lambda_val\n",
    "        \n",
    "        # Initialize weights and biases (He initialization)\n",
    "        self.W = [np.random.randn(layer_sizes[i+1], layer_sizes[i]) * np.sqrt(2./layer_sizes[i]) \n",
    "                  for i in range(self.num_layers-1)]\n",
    "        self.b = [np.random.randn(layer_sizes[i+1], 1)*0 for i in range(self.num_layers-1)]\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        a = [x]\n",
    "        z = []\n",
    "        \n",
    "        # Hidden layers\n",
    "        for i in range(self.num_layers-2):\n",
    "            z_i = self.W[i].dot(a[i]) + self.b[i]\n",
    "            a_i = self.activation_fn(z_i)\n",
    "            z.append(z_i)\n",
    "            a.append(a_i)\n",
    "        \n",
    "        # Output layer\n",
    "        z_out = self.W[-1].dot(a[-1]) + self.b[-1]\n",
    "        a_out = self.output_activation_fn(z_out)\n",
    "        z.append(z_out)\n",
    "        a.append(a_out)\n",
    "        \n",
    "        return a_out, a, z\n",
    "    \n",
    "    def compute_loss(self, predictions, t):\n",
    "        # Ensure the target shape matches the predictions\n",
    "        if t.shape[0] != predictions.shape[0]:\n",
    "            t = t.T\n",
    "\n",
    "        # Original loss without regularization\n",
    "        loss_without_reg = self.loss_fn(predictions, t)\n",
    "\n",
    "        # L1 regularization penalty\n",
    "        l1_penalty = sum([abs(w).sum() for w in self.W])\n",
    "\n",
    "        # Total loss with regularization\n",
    "        total_loss = loss_without_reg + self.lambda_val * l1_penalty\n",
    "\n",
    "        return total_loss\n",
    "    \n",
    "    def _one_hot(self, inputs):\n",
    "        n_values = len(np.unique(inputs))\n",
    "        return np.eye(n_values)[:,inputs]\n",
    "    \n",
    "    def backward(self, x, t, a, z):\n",
    "        m = x.shape[1]\n",
    "        dL_dw = [np.zeros_like(w) for w in self.W]\n",
    "        dL_db = [np.zeros_like(b) for b in self.b]\n",
    "        \n",
    "        # Ensure the target shape matches the predictions\n",
    "        if t.shape[0] != a[-1].shape[0]:\n",
    "            t = t.T\n",
    "        \n",
    "        dL_da = self.loss_fn_deriv(a[-1], t)\n",
    "        dL_dz = dL_da * self.output_activation_fn_deriv(z[-1])\n",
    "        \n",
    "        dL_dw[-1] = dL_dz.dot(a[-2].T)\n",
    "        dL_db[-1] = np.sum(dL_dz, axis=1, keepdims=True)\n",
    "        \n",
    "        for i in range(self.num_layers-3, -1, -1):\n",
    "            dL_da = self.W[i+1].T.dot(dL_dz)\n",
    "            dL_dz = dL_da * self.activation_fn_deriv(z[i])\n",
    "            dL_dw[i] = dL_dz.dot(a[i].T)\n",
    "            dL_db[i] = np.sum(dL_dz, axis=1, keepdims=True)\n",
    "        \n",
    "        return dL_dw, dL_db\n",
    "    \n",
    "    def update_weights(self, dL_dw, dL_db, learning_rate): #with L1 regularization\n",
    "        # Adjust the gradient for L1 regularization\n",
    "        dL_dw_regularized = [dw + self.lambda_val * np.sign(w) for w, dw in zip(self.W, dL_dw)]\n",
    "\n",
    "        # Update weights and biases using the adjusted gradient\n",
    "        self.W = [w - learning_rate * dw_reg for w, dw_reg in zip(self.W, dL_dw_regularized)]\n",
    "        self.b = [b - learning_rate * db for b, db in zip(self.b, dL_db)]\n",
    "        \n",
    "    def train(self, X, labels, learning_rate=0.01, epochs=1000, batch_size=None):\n",
    "        \"\"\"Trains the neural network using the given training data and labels.\"\"\"\n",
    "        m = X.shape[1]\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            if batch_size:  # If batch size is specified, use mini-batch gradient descent\n",
    "                num_batches = m // batch_size\n",
    "                for batch in range(num_batches):\n",
    "                    X_batch = X[:, batch*batch_size:(batch+1)*batch_size]\n",
    "                    labels_batch = labels[batch*batch_size:(batch+1)*batch_size]\n",
    "\n",
    "                    # Forward pass\n",
    "                    predictions, a_batch, z_batch = self.forward(X_batch)\n",
    "                    \n",
    "                    # Backward pass\n",
    "                    dL_dw, dL_db = self.backward(X_batch, labels_batch, a_batch, z_batch)\n",
    "\n",
    "                    # Update weights and biases\n",
    "                    self.update_weights(dL_dw, dL_db, learning_rate)\n",
    "            else:  # Otherwise, use batch gradient descent\n",
    "                # Forward pass\n",
    "                predictions, a_full, z_full = self.forward(X)\n",
    "                \n",
    "                # Backward pass\n",
    "                dL_dw, dL_db = self.backward(X, labels, a_full, z_full)\n",
    "\n",
    "                # Update weights and biases\n",
    "                self.update_weights(dL_dw, dL_db, learning_rate)\n",
    "\n",
    "            # Print loss at the end of each 100 epochs:\n",
    "            if epoch % 100 == 0:\n",
    "                # Ensure activations are for the entire dataset\n",
    "                predictions, _, _ = self.forward(X)\n",
    "                loss = self.compute_loss(predictions, labels)\n",
    "                print(f\"Epoch {epoch+1}/{epochs} - Loss: {np.mean(loss)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1f9121a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU(inputs):\n",
    "    return np.maximum(inputs, 0)\n",
    "def deriv_ReLU(Z):\n",
    "    return Z>0\n",
    "\n",
    "def sigmoid(inputs, clip_value=200):\n",
    "    inputs = np.clip(inputs, -clip_value, clip_value)\n",
    "    return 1 / (1 + np.exp(-inputs))\n",
    "\n",
    "def deriv_sigmoid(inputs):\n",
    "    return(inputs*(1-inputs))\n",
    "\n",
    "def Linear(inputs):\n",
    "    return inputs\n",
    "\n",
    "def deriv_Linear(inputs):\n",
    "    return inputs*0 + 1\n",
    "\n",
    "def clipped_ReLU(x, c=1):\n",
    "    return np.minimum(np.maximum(0, x), c)\n",
    "\n",
    "def deriv_clipped_ReLU(inputs):\n",
    "    return (inputs>0) & (inputs<1)\n",
    "\n",
    "def L2(outputs, targets):\n",
    "    return 0.5*sum((outputs - targets)**2)\n",
    "\n",
    "# L1 regularization\n",
    "def L2_regularized(outputs, targets, weights, lambda_val):\n",
    "    l1_penalty = sum([abs(w).sum() for w in weights])\n",
    "    return L2(outputs, targets) + lambda_val * l1_penalty\n",
    "\n",
    "def deriv_L2(outputs, targets):\n",
    "    return outputs - targets\n",
    "\n",
    "def one_hot(inputs):\n",
    "    n_values = np.max(inputs) + 1\n",
    "    return np.eye(n_values)[:,inputs]\n",
    "\n",
    "def softmax(Z):\n",
    "    shiftZ = Z - np.max(Z, axis=0)\n",
    "    exps = np.exp(shiftZ)\n",
    "    return exps / np.sum(exps, axis=0)\n",
    "\n",
    "def dummy_deriv_softmax(Z):\n",
    "    return Z\n",
    "\n",
    "def categorical_cross_entropy(predictions, labels):\n",
    "    m = labels.shape[1]\n",
    "    return -np.sum(np.log(predictions) * labels) / m\n",
    "\n",
    "def deriv_cat_cross_entropy(predictions, labels):\n",
    "    return predictions - labels  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "20e5fb5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20000 - Loss: 166.51873817407832\n",
      "Epoch 101/20000 - Loss: 0.05284816859230499\n",
      "Epoch 201/20000 - Loss: 0.022261671661424863\n",
      "Epoch 301/20000 - Loss: 0.011102190067582316\n",
      "Epoch 401/20000 - Loss: 0.007133921426455541\n",
      "Epoch 501/20000 - Loss: 0.0057511664761877215\n",
      "Epoch 601/20000 - Loss: 0.005276911565550089\n",
      "Epoch 701/20000 - Loss: 0.005116768022213965\n",
      "Epoch 801/20000 - Loss: 0.005063778810973689\n",
      "Epoch 901/20000 - Loss: 0.005046813504791723\n",
      "Epoch 1001/20000 - Loss: 0.005041710526051425\n",
      "Epoch 1101/20000 - Loss: 0.005040378102920519\n",
      "Epoch 1201/20000 - Loss: 0.005040164109215569\n",
      "Epoch 1301/20000 - Loss: 0.0050402318681317315\n",
      "Epoch 1401/20000 - Loss: 0.005040336944239601\n",
      "Epoch 1501/20000 - Loss: 0.005040420236203965\n",
      "Epoch 1601/20000 - Loss: 0.0050404758739295875\n",
      "Epoch 1701/20000 - Loss: 0.0050405102395024055\n",
      "Epoch 1801/20000 - Loss: 0.005040530421697264\n",
      "Epoch 1901/20000 - Loss: 0.005040541713652651\n",
      "Epoch 2001/20000 - Loss: 0.00504054760308402\n",
      "Epoch 2101/20000 - Loss: 0.005040550263658572\n",
      "Epoch 2201/20000 - Loss: 0.005040551012321261\n",
      "Epoch 2301/20000 - Loss: 0.005040550634845766\n",
      "Epoch 2401/20000 - Loss: 0.0050405495960769015\n",
      "Epoch 2501/20000 - Loss: 0.005040548169665918\n",
      "Epoch 2601/20000 - Loss: 0.005040546516256787\n",
      "Epoch 2701/20000 - Loss: 0.005040544730000025\n",
      "Epoch 2801/20000 - Loss: 0.005040542866023149\n",
      "Epoch 2901/20000 - Loss: 0.005040540956586857\n",
      "Epoch 3001/20000 - Loss: 0.00504053902056406\n",
      "Epoch 3101/20000 - Loss: 0.005040537068993562\n",
      "Epoch 3201/20000 - Loss: 0.005040535108331265\n",
      "Epoch 3301/20000 - Loss: 0.0050405331423525705\n",
      "Epoch 3401/20000 - Loss: 0.005040531173265272\n",
      "Epoch 3501/20000 - Loss: 0.005040529202360393\n",
      "Epoch 3601/20000 - Loss: 0.005040527230392862\n",
      "Epoch 3701/20000 - Loss: 0.005040525257804144\n",
      "Epoch 3801/20000 - Loss: 0.005040523284852379\n",
      "Epoch 3901/20000 - Loss: 0.005040521311688511\n",
      "Epoch 4001/20000 - Loss: 0.005040519338400806\n",
      "Epoch 4101/20000 - Loss: 0.005040517365040877\n",
      "Epoch 4201/20000 - Loss: 0.005040515391638905\n",
      "Epoch 4301/20000 - Loss: 0.005040513418212541\n",
      "Epoch 4401/20000 - Loss: 0.005040511444772103\n",
      "Epoch 4501/20000 - Loss: 0.005040509471323629\n",
      "Epoch 4601/20000 - Loss: 0.005040507497870642\n",
      "Epoch 4701/20000 - Loss: 0.005040505524415204\n",
      "Epoch 4801/20000 - Loss: 0.005040503550958542\n",
      "Epoch 4901/20000 - Loss: 0.005040501577501319\n",
      "Epoch 5001/20000 - Loss: 0.00504049960404399\n",
      "Epoch 5101/20000 - Loss: 0.005040497630586812\n",
      "Epoch 5201/20000 - Loss: 0.0050404956571299215\n",
      "Epoch 5301/20000 - Loss: 0.005040493683673353\n",
      "Epoch 5401/20000 - Loss: 0.005040491710217151\n",
      "Epoch 5501/20000 - Loss: 0.00504048973676137\n",
      "Epoch 5601/20000 - Loss: 0.005040487763306028\n",
      "Epoch 5701/20000 - Loss: 0.005040485789851116\n",
      "Epoch 5801/20000 - Loss: 0.005040483816396642\n",
      "Epoch 5901/20000 - Loss: 0.005040481842942624\n",
      "Epoch 6001/20000 - Loss: 0.0050404798694890715\n",
      "Epoch 6101/20000 - Loss: 0.00504047789603594\n",
      "Epoch 6201/20000 - Loss: 0.005040475922583283\n",
      "Epoch 6301/20000 - Loss: 0.005040473949131109\n",
      "Epoch 6401/20000 - Loss: 0.005040471975679409\n",
      "Epoch 6501/20000 - Loss: 0.00504047000222815\n",
      "Epoch 6601/20000 - Loss: 0.005040468028777327\n",
      "Epoch 6701/20000 - Loss: 0.005040466055326989\n",
      "Epoch 6801/20000 - Loss: 0.005040464081877111\n",
      "Epoch 6901/20000 - Loss: 0.005040462108427704\n",
      "Epoch 7001/20000 - Loss: 0.005040460134978717\n",
      "Epoch 7101/20000 - Loss: 0.0050404581615301905\n",
      "Epoch 7201/20000 - Loss: 0.005040456188082122\n",
      "Epoch 7301/20000 - Loss: 0.005040454214634521\n",
      "Epoch 7401/20000 - Loss: 0.005040452241187392\n",
      "Epoch 7501/20000 - Loss: 0.005040450267740691\n",
      "Epoch 7601/20000 - Loss: 0.0050404482942944554\n",
      "Epoch 7701/20000 - Loss: 0.005040446320848671\n",
      "Epoch 7801/20000 - Loss: 0.00504044434740337\n",
      "Epoch 7901/20000 - Loss: 0.005040442373958574\n",
      "Epoch 8001/20000 - Loss: 0.005040440400514236\n",
      "Epoch 8101/20000 - Loss: 0.005040438427070326\n",
      "Epoch 8201/20000 - Loss: 0.005040436453626857\n",
      "Epoch 8301/20000 - Loss: 0.005040434480183842\n",
      "Epoch 8401/20000 - Loss: 0.0050404325067413235\n",
      "Epoch 8501/20000 - Loss: 0.005040430533299262\n",
      "Epoch 8601/20000 - Loss: 0.005040428559857631\n",
      "Epoch 8701/20000 - Loss: 0.005040426586416418\n",
      "Epoch 8801/20000 - Loss: 0.005040424612975654\n",
      "Epoch 8901/20000 - Loss: 0.005040422639535392\n",
      "Epoch 9001/20000 - Loss: 0.005040420666095601\n",
      "Epoch 9101/20000 - Loss: 0.005040418692656219\n",
      "Epoch 9201/20000 - Loss: 0.005040416719217319\n",
      "Epoch 9301/20000 - Loss: 0.005040414745778866\n",
      "Epoch 9401/20000 - Loss: 0.005040412772340905\n",
      "Epoch 9501/20000 - Loss: 0.005040410798903416\n",
      "Epoch 9601/20000 - Loss: 0.005040408825466355\n",
      "Epoch 9701/20000 - Loss: 0.005040406852029793\n",
      "Epoch 9801/20000 - Loss: 0.005040404878593634\n",
      "Epoch 9901/20000 - Loss: 0.005040402905157934\n",
      "Epoch 10001/20000 - Loss: 0.005040400931722699\n",
      "Epoch 10101/20000 - Loss: 0.005040398958287959\n",
      "Epoch 10201/20000 - Loss: 0.005040396984853629\n",
      "Epoch 10301/20000 - Loss: 0.005040395011419762\n",
      "Epoch 10401/20000 - Loss: 0.005040393037986371\n",
      "Epoch 10501/20000 - Loss: 0.005040391064553464\n",
      "Epoch 10601/20000 - Loss: 0.005040389091121018\n",
      "Epoch 10701/20000 - Loss: 0.005040387117688992\n",
      "Epoch 10801/20000 - Loss: 0.005040385144257441\n",
      "Epoch 10901/20000 - Loss: 0.00504038317082636\n",
      "Epoch 11001/20000 - Loss: 0.005040381197395745\n",
      "Epoch 11101/20000 - Loss: 0.0050403792239655555\n",
      "Epoch 11201/20000 - Loss: 0.005040377250535841\n",
      "Epoch 11301/20000 - Loss: 0.00504037527710657\n",
      "Epoch 11401/20000 - Loss: 0.005040373303677745\n",
      "Epoch 11501/20000 - Loss: 0.0050403713302494\n",
      "Epoch 11601/20000 - Loss: 0.00504036935682148\n",
      "Epoch 11701/20000 - Loss: 0.005040367383394033\n",
      "Epoch 11801/20000 - Loss: 0.0050403654099670445\n",
      "Epoch 11901/20000 - Loss: 0.005040363436540542\n",
      "Epoch 12001/20000 - Loss: 0.005040361463114492\n",
      "Epoch 12101/20000 - Loss: 0.005040359489688866\n",
      "Epoch 12201/20000 - Loss: 0.005040357516263733\n",
      "Epoch 12301/20000 - Loss: 0.005040355542839042\n",
      "Epoch 12401/20000 - Loss: 0.0050403535694147975\n",
      "Epoch 12501/20000 - Loss: 0.005040351595991022\n",
      "Epoch 12601/20000 - Loss: 0.005040349622567736\n",
      "Epoch 12701/20000 - Loss: 0.0050403476491448895\n",
      "Epoch 12801/20000 - Loss: 0.005040345675722493\n",
      "Epoch 12901/20000 - Loss: 0.005040343702300531\n",
      "Epoch 13001/20000 - Loss: 0.005040341728879061\n",
      "Epoch 13101/20000 - Loss: 0.005040339755458049\n",
      "Epoch 13201/20000 - Loss: 0.00504033778203747\n",
      "Epoch 13301/20000 - Loss: 0.005040335808617371\n",
      "Epoch 13401/20000 - Loss: 0.005040333835197729\n",
      "Epoch 13501/20000 - Loss: 0.005040331861778547\n",
      "Epoch 13601/20000 - Loss: 0.005040329888359804\n",
      "Epoch 13701/20000 - Loss: 0.0050403279149415225\n",
      "Epoch 13801/20000 - Loss: 0.005040325941523711\n",
      "Epoch 13901/20000 - Loss: 0.005040323968106332\n",
      "Epoch 14001/20000 - Loss: 0.005040321994689423\n",
      "Epoch 14101/20000 - Loss: 0.005040320021272984\n",
      "Epoch 14201/20000 - Loss: 0.005040318047857015\n",
      "Epoch 14301/20000 - Loss: 0.005040316074441492\n",
      "Epoch 14401/20000 - Loss: 0.0050403141010264135\n",
      "Epoch 14501/20000 - Loss: 0.0050403121276118065\n",
      "Epoch 14601/20000 - Loss: 0.005040310154197678\n",
      "Epoch 14701/20000 - Loss: 0.005040308180784016\n",
      "Epoch 14801/20000 - Loss: 0.0050403062073707955\n",
      "Epoch 14901/20000 - Loss: 0.005040304233958058\n",
      "Epoch 15001/20000 - Loss: 0.005040302260545757\n",
      "Epoch 15101/20000 - Loss: 0.005040300287133928\n",
      "Epoch 15201/20000 - Loss: 0.005040298313722529\n",
      "Epoch 15301/20000 - Loss: 0.005040296340311581\n",
      "Epoch 15401/20000 - Loss: 0.005040294366901104\n",
      "Epoch 15501/20000 - Loss: 0.0050402923934910965\n",
      "Epoch 15601/20000 - Loss: 0.005040290420081576\n",
      "Epoch 15701/20000 - Loss: 0.00504028844667247\n",
      "Epoch 15801/20000 - Loss: 0.005040286473263798\n",
      "Epoch 15901/20000 - Loss: 0.005040284499855587\n",
      "Epoch 16001/20000 - Loss: 0.005040282526447877\n",
      "Epoch 16101/20000 - Loss: 0.005040280553040642\n",
      "Epoch 16201/20000 - Loss: 0.005040278579633824\n",
      "Epoch 16301/20000 - Loss: 0.005040276606227478\n",
      "Epoch 16401/20000 - Loss: 0.005040274632821571\n",
      "Epoch 16501/20000 - Loss: 0.005040272659416159\n",
      "Epoch 16601/20000 - Loss: 0.005040270686011168\n",
      "Epoch 16701/20000 - Loss: 0.005040268712606663\n",
      "Epoch 16801/20000 - Loss: 0.005040266739202593\n",
      "Epoch 16901/20000 - Loss: 0.005040264765798956\n",
      "Epoch 17001/20000 - Loss: 0.0050402627923957985\n",
      "Epoch 17101/20000 - Loss: 0.005040260818993148\n",
      "Epoch 17201/20000 - Loss: 0.005040258845591003\n",
      "Epoch 17301/20000 - Loss: 0.00504025687218925\n",
      "Epoch 17401/20000 - Loss: 0.005040254898787931\n",
      "Epoch 17501/20000 - Loss: 0.005040252925387064\n",
      "Epoch 17601/20000 - Loss: 0.005040250951986703\n",
      "Epoch 17701/20000 - Loss: 0.00504024897858679\n",
      "Epoch 17801/20000 - Loss: 0.005040247005187313\n",
      "Epoch 17901/20000 - Loss: 0.005040245031788274\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18001/20000 - Loss: 0.005040243058389683\n",
      "Epoch 18101/20000 - Loss: 0.005040241084991562\n",
      "Epoch 18201/20000 - Loss: 0.005040239111593907\n",
      "Epoch 18301/20000 - Loss: 0.005040237138196696\n",
      "Epoch 18401/20000 - Loss: 0.005040235164799925\n",
      "Epoch 18501/20000 - Loss: 0.0050402331914036255\n",
      "Epoch 18601/20000 - Loss: 0.0050402312180078054\n",
      "Epoch 18701/20000 - Loss: 0.005040229244612472\n",
      "Epoch 18801/20000 - Loss: 0.0050402272712176\n",
      "Epoch 18901/20000 - Loss: 0.005040225297823174\n",
      "Epoch 19001/20000 - Loss: 0.005040223324429211\n",
      "Epoch 19101/20000 - Loss: 0.005040221351035676\n",
      "Epoch 19201/20000 - Loss: 0.005040219377642624\n",
      "Epoch 19301/20000 - Loss: 0.005040217404250051\n",
      "Epoch 19401/20000 - Loss: 0.005040215430857908\n",
      "Epoch 19501/20000 - Loss: 0.005040213457466189\n",
      "Epoch 19601/20000 - Loss: 0.005040211484074948\n",
      "Epoch 19701/20000 - Loss: 0.005040209510684195\n",
      "Epoch 19801/20000 - Loss: 0.005040207537293899\n",
      "Epoch 19901/20000 - Loss: 0.00504020556390405\n"
     ]
    }
   ],
   "source": [
    "layer_sizes = [1,2,1]\n",
    "\n",
    "ffnn = FFNN(layer_sizes = layer_sizes, \n",
    "                activation_fn = Linear, \n",
    "                activation_fn_deriv = deriv_Linear, \n",
    "                output_activation_fn = Linear, \n",
    "                output_activation_fn_deriv = deriv_Linear, \n",
    "                loss_fn = L2, \n",
    "                loss_fn_deriv = deriv_L2,\n",
    "                task_type='regression',\n",
    "                lambda_val=0.001)\n",
    "\n",
    "ffnn.train(X.T, Y, learning_rate=0.000001, epochs = 20000, batch_size=700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "ab843a8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.29196784 0.30996519 0.61998636 0.66483295 0.70785362 0.76425623\n",
      "  0.16574462 0.93456168 0.68778562 0.7586549 ]]\n",
      "[[-17.61557229 -17.60557385 -17.43334125 -17.40842668 -17.3845265\n",
      "  -17.35319197 -17.68569574 -17.25857858 -17.39567529 -17.35630379]]\n"
     ]
    }
   ],
   "source": [
    "'''check the conversion capabilities'''\n",
    "far_set = X[11:21]\n",
    "# -17.61557342222\n",
    "cel_set,_,_ = ffnn.forward(far_set.T)\n",
    "print(far_set.T)\n",
    "print(cel_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "d0d12254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[0.22621464],\n",
      "       [0.40600997]]), array([[ 3.70949574, -0.69848325]])]\n",
      "[array([[-3.94901042],\n",
      "       [ 0.59814947]]), array([[-2.7111406]])]\n"
     ]
    }
   ],
   "source": [
    "''' print weights and biases to begin grokking the network '''\n",
    "print(ffnn.W)\n",
    "print(ffnn.b)\n",
    "w11, w12, w21, w22 = (ffnn.W[0][0][0], ffnn.W[0][1][0], ffnn.W[1][0][0], ffnn.W[1][0][1])\n",
    "bh1, bh2, bo = (ffnn.b[0][0][0], ffnn.b[0][1][0], ffnn.b[1][0][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "fd919e05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-20.555530784630438\n",
      "-17.777775344444525\n",
      "-15.000019904258611\n",
      "-0.00014052725467905702\n"
     ]
    }
   ],
   "source": [
    "def learned_conversion(f):\n",
    "    h1 = w11 * f + bh1\n",
    "    h2 = w12 * f + bh2\n",
    "    o = w21 * h1 + w22 * h2 + bo\n",
    "    return o\n",
    "\n",
    "temps = [-5, 0, 5, 32]\n",
    "for t in temps:\n",
    "    print(learned_conversion(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "a5b16cc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learned slope =  0.5555510880371828\n",
      "learned intercept =  -17.777775344444525\n",
      "actual slope =  0.5555555555555556\n",
      "actual intercept =  -17.77777777777778\n"
     ]
    }
   ],
   "source": [
    "'''learned_conversion is equivalent to the following:\n",
    "(w21*w11 + w22*w12)*f + (w21*bh1 + w22*bh2 + bo) '''\n",
    "\n",
    "print(\"learned slope = \", (w21*w11 + w22*w12))\n",
    "print(\"learned intercept = \", (w21*bh1 + w22*bh2 + bo))\n",
    "\n",
    "'''actual formula is (f - 32) * 5/9 \n",
    "aka, (5/9)*f - 32*(5/9)'''\n",
    "\n",
    "print(\"actual slope = \", 5/9)\n",
    "print(\"actual intercept = \", (5/9)*-32)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "345.352px",
    "left": "1410.98px",
    "right": "20px",
    "top": "69.9707px",
    "width": "379.482px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
