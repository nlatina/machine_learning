{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1ad78c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "b6875182",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''helper functions for data generation'''\n",
    "def rgb_to_hsv(rgb):\n",
    "    r, g, b = rgb[..., 0], rgb[..., 1], rgb[..., 2]\n",
    "    maxc = np.maximum(r, np.maximum(g, b))\n",
    "    minc = np.minimum(r, np.minimum(g, b))\n",
    "    v = maxc\n",
    "    deltac = maxc - minc\n",
    "    s = deltac / maxc\n",
    "    s = np.where(maxc == 0, 0, s)\n",
    "    deltac_recip = 1 / deltac\n",
    "    rc = (maxc - r) * deltac_recip\n",
    "    gc = (maxc - g) * deltac_recip\n",
    "    bc = (maxc - b) * deltac_recip\n",
    "    h = np.where(maxc == r, bc - gc,\n",
    "                 np.where(maxc == g, 2.0 + rc - bc, 4.0 + gc - rc))\n",
    "    h = (h / 6.0) % 1.0\n",
    "    return np.stack([h, s, v], axis=-1)\n",
    "\n",
    "def hsv_to_rgb(hsv):\n",
    "    h, s, v = hsv[..., 0], hsv[..., 1], hsv[..., 2]\n",
    "    i = (h * 6).astype(np.int32)\n",
    "    f = h * 6 - i\n",
    "    p = v * (1 - s)\n",
    "    q = v * (1 - s * f)\n",
    "    t = v * (1 - s * (1 - f))\n",
    "    i = i % 6\n",
    "\n",
    "    conditions = [s == 0, i == 0, i == 1, i == 2, i == 3, i == 4, i == 5]\n",
    "    rgb_lists = [\n",
    "        (v, v, v),\n",
    "        (v, t, p),\n",
    "        (q, v, p),\n",
    "        (p, v, t),\n",
    "        (p, q, v),\n",
    "        (t, p, v),\n",
    "        (v, p, q)\n",
    "    ]\n",
    "\n",
    "    r, g, b = np.select(conditions, rgb_lists)\n",
    "    return np.stack([r, g, b], axis=-1)\n",
    "\n",
    "def generate_data(n):\n",
    "    rgb_colors = np.random.rand(n, 3)\n",
    "    hsv_colors = rgb_to_hsv(rgb_colors)\n",
    "    return rgb_colors, hsv_colors\n",
    "\n",
    "def plot_colors(rgb_list, hsv_list):\n",
    "    # Ensure that the inputs are 2D arrays (i.e., lists of colors)\n",
    "    if len(np.shape(rgb_list)) == 1:\n",
    "        rgb_list = np.expand_dims(rgb_list, axis=0)\n",
    "    if len(np.shape(hsv_list)) == 1:\n",
    "        hsv_list = np.expand_dims(hsv_list, axis=0)\n",
    "    \n",
    "    # Create empty list to store patches\n",
    "    patches = []\n",
    "\n",
    "    # For each RGB and HSV pair\n",
    "    for rgb, hsv in zip(rgb_list, hsv_list):\n",
    "        # Convert HSV to RGB\n",
    "        rgb_from_hsv = hsv_to_rgb(hsv)\n",
    "        \n",
    "        # Scale to 255 for visualization\n",
    "        rgb_patch = (rgb * 255).astype(np.uint8)\n",
    "        hsv_patch = (rgb_from_hsv * 255).astype(np.uint8)\n",
    "\n",
    "        # Create a vertical patch for each color\n",
    "        combined_patch = np.vstack((np.ones((50, 100, 3), dtype=np.uint8) * rgb_patch,\n",
    "                                    np.ones((50, 100, 3), dtype=np.uint8) * hsv_patch))\n",
    "        patches.append(combined_patch)\n",
    "\n",
    "    # Combine all patches horizontally\n",
    "    combined_image = np.hstack(patches)\n",
    "\n",
    "    # Display using matplotlib\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.imshow(combined_image)\n",
    "    plt.title(\"Colors: RGB (Top) vs. HSV (Bottom)\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "X, Y = generate_data(20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "398f899e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_class_data(n):\n",
    "    dataset = []\n",
    "    labels = []\n",
    "    X = np.random.uniform(0,1,(n,2))\n",
    "    for point in X:\n",
    "        if point[1] < 0.5 and point[1] < (-1/0.5)*point[0] + 1:\n",
    "            labels.append(0)\n",
    "        else:\n",
    "            labels.append(1)\n",
    "            \n",
    "    return X, np.array(labels).astype('int')\n",
    "X, labels = generate_class_data(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "4bd7a4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFNN:\n",
    "    def __init__(self, layer_sizes, \n",
    "                 activation_fn, activation_fn_deriv, \n",
    "                 output_activation_fn, output_activation_fn_deriv, \n",
    "                 loss_fn, loss_fn_deriv, task_type='classification'):\n",
    "        self.num_layers = len(layer_sizes)\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.activation_fn = activation_fn\n",
    "        self.activation_fn_deriv = activation_fn_deriv\n",
    "        self.output_activation_fn = output_activation_fn\n",
    "        self.output_activation_fn_deriv = output_activation_fn_deriv\n",
    "        self.loss_fn = loss_fn\n",
    "        self.loss_fn_deriv = loss_fn_deriv\n",
    "        self.task_type = task_type\n",
    "        \n",
    "        # Initialize weights and biases\n",
    "        self.W = [np.random.randn(layer_sizes[i+1], layer_sizes[i]) * np.sqrt(2./layer_sizes[i]) \n",
    "                  for i in range(self.num_layers-1)]\n",
    "        self.b = [np.random.randn(layer_sizes[i+1], 1) for i in range(self.num_layers-1)]\n",
    "\n",
    "    def forward(self, x):\n",
    "        a = [x]\n",
    "        z = []\n",
    "        \n",
    "        # Hidden layers\n",
    "        for i in range(self.num_layers-2):\n",
    "            z_i = self.W[i].dot(a[i]) + self.b[i]\n",
    "            a_i = self.activation_fn(z_i)\n",
    "            z.append(z_i)\n",
    "            a.append(a_i)\n",
    "        \n",
    "        # Output layer\n",
    "        z_out = self.W[-1].dot(a[-1]) + self.b[-1]\n",
    "        a_out = self.output_activation_fn(z_out)\n",
    "        z.append(z_out)\n",
    "        a.append(a_out)\n",
    "        \n",
    "        return a_out, a, z\n",
    "    \n",
    "    def compute_loss(self, predictions, t):\n",
    "        # Ensure the target shape matches the predictions\n",
    "        if t.shape[0] != predictions.shape[0]:\n",
    "            t = t.T\n",
    "        return self.loss_fn(predictions, t)\n",
    "    \n",
    "    def _one_hot(self, inputs):\n",
    "        n_values = len(np.unique(inputs))\n",
    "        return np.eye(n_values)[:,inputs]\n",
    "    \n",
    "    def backward(self, x, t, a, z):\n",
    "        m = x.shape[1]\n",
    "        dL_dw = [np.zeros_like(w) for w in self.W]\n",
    "        dL_db = [np.zeros_like(b) for b in self.b]\n",
    "        \n",
    "        # Ensure the target shape matches the predictions\n",
    "        if t.shape[0] != a[-1].shape[0]:\n",
    "            t = t.T\n",
    "        \n",
    "        dL_da = self.loss_fn_deriv(a[-1], t)\n",
    "        dL_dz = dL_da * self.output_activation_fn_deriv(z[-1])\n",
    "        \n",
    "        dL_dw[-1] = dL_dz.dot(a[-2].T)\n",
    "        dL_db[-1] = np.sum(dL_dz, axis=1, keepdims=True)\n",
    "        \n",
    "        for i in range(self.num_layers-3, -1, -1):\n",
    "            dL_da = self.W[i+1].T.dot(dL_dz)\n",
    "            dL_dz = dL_da * self.activation_fn_deriv(z[i])\n",
    "            dL_dw[i] = dL_dz.dot(a[i].T)\n",
    "            dL_db[i] = np.sum(dL_dz, axis=1, keepdims=True)\n",
    "        \n",
    "        return dL_dw, dL_db\n",
    "    \n",
    "    def update_weights(self, dL_dw, dL_db, learning_rate):\n",
    "        self.W = [w - learning_rate * dw for w, dw in zip(self.W, dL_dw)]\n",
    "        self.b = [b - learning_rate * db for b, db in zip(self.b, dL_db)]\n",
    "        \n",
    "    def train(self, X, labels, learning_rate=0.01, epochs=1000, batch_size=None):\n",
    "        \"\"\"Trains the neural network using the given training data and labels.\"\"\"\n",
    "        m = X.shape[1]\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            if batch_size:  # If batch size is specified, use mini-batch gradient descent\n",
    "                num_batches = m // batch_size\n",
    "                for batch in range(num_batches):\n",
    "                    X_batch = X[:, batch*batch_size:(batch+1)*batch_size]\n",
    "                    labels_batch = labels[batch*batch_size:(batch+1)*batch_size]\n",
    "\n",
    "                    # Forward pass\n",
    "                    predictions, a_batch, z_batch = self.forward(X_batch)\n",
    "                    \n",
    "                    # Backward pass\n",
    "                    dL_dw, dL_db = self.backward(X_batch, labels_batch, a_batch, z_batch)\n",
    "\n",
    "                    # Update weights and biases\n",
    "                    self.update_weights(dL_dw, dL_db, learning_rate)\n",
    "            else:  # Otherwise, use batch gradient descent\n",
    "                # Forward pass\n",
    "                predictions, a_full, z_full = self.forward(X)\n",
    "                \n",
    "                # Backward pass\n",
    "                dL_dw, dL_db = self.backward(X, labels, a_full, z_full)\n",
    "\n",
    "                # Update weights and biases\n",
    "                self.update_weights(dL_dw, dL_db, learning_rate)\n",
    "\n",
    "            # Print loss at the end of each 100 epochs:\n",
    "            if epoch % 100 == 0:\n",
    "                # Ensure activations are for the entire dataset\n",
    "                predictions, _, _ = self.forward(X)\n",
    "                loss = self.compute_loss(predictions, labels)\n",
    "                print(f\"Epoch {epoch+1}/{epochs} - Loss: {np.mean(loss)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "1f9121a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU(inputs):\n",
    "    return np.maximum(inputs, 0)\n",
    "def deriv_ReLU(Z):\n",
    "    return Z>0\n",
    "\n",
    "def sigmoid(inputs, clip_value=200):\n",
    "    inputs = np.clip(inputs, -clip_value, clip_value)\n",
    "    return 1 / (1 + np.exp(-inputs))\n",
    "\n",
    "def deriv_sigmoid(inputs):\n",
    "    return(inputs*(1-inputs))\n",
    "\n",
    "def Linear(inputs):\n",
    "    return (1/10)*inputs\n",
    "\n",
    "def deriv_Linear(inputs):\n",
    "    return inputs*0 + (1/10)\n",
    "\n",
    "def clipped_ReLU(x, c=1):\n",
    "    return np.minimum(np.maximum(0, x), c)\n",
    "\n",
    "def deriv_clipped_ReLU(inputs):\n",
    "    return (inputs>0) & (inputs<1)\n",
    "\n",
    "def L2(outputs, targets):\n",
    "    return 0.5*sum((outputs - targets)**2)\n",
    "\n",
    "def deriv_L2(outputs, targets):\n",
    "    return outputs - targets\n",
    "\n",
    "def one_hot(inputs):\n",
    "    n_values = np.max(inputs) + 1\n",
    "    return np.eye(n_values)[:,inputs]\n",
    "\n",
    "def softmax(Z):\n",
    "    shiftZ = Z - np.max(Z, axis=0)\n",
    "    exps = np.exp(shiftZ)\n",
    "    return exps / np.sum(exps, axis=0)\n",
    "\n",
    "def dummy_deriv_softmax(Z):\n",
    "    return Z\n",
    "\n",
    "def categorical_cross_entropy(predictions, labels):\n",
    "    m = labels.shape[1]\n",
    "    return -np.sum(np.log(predictions) * labels) / m\n",
    "\n",
    "def deriv_cat_cross_entropy(predictions, labels):\n",
    "    return predictions - labels  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "20e5fb5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3000 - Loss: 0.08516999923072092\n",
      "Epoch 101/3000 - Loss: 0.021990920592541124\n",
      "Epoch 201/3000 - Loss: 0.01871483160516246\n",
      "Epoch 301/3000 - Loss: 0.016106605696559476\n",
      "Epoch 401/3000 - Loss: 0.013604489252377698\n",
      "Epoch 501/3000 - Loss: 0.011519789654065346\n",
      "Epoch 601/3000 - Loss: 0.010041862732917517\n",
      "Epoch 701/3000 - Loss: 0.009014748215954762\n",
      "Epoch 801/3000 - Loss: 0.008223171526960046\n",
      "Epoch 901/3000 - Loss: 0.00759061258473389\n",
      "Epoch 1001/3000 - Loss: 0.007162305024142605\n",
      "Epoch 1101/3000 - Loss: 0.007575754851592386\n",
      "Epoch 1201/3000 - Loss: 0.007357665456477429\n",
      "Epoch 1301/3000 - Loss: 0.007112143114320328\n",
      "Epoch 1401/3000 - Loss: 0.006821647316424006\n",
      "Epoch 1501/3000 - Loss: 0.006504974607482655\n",
      "Epoch 1601/3000 - Loss: 0.006298831113043022\n",
      "Epoch 1701/3000 - Loss: 0.006186581411074203\n",
      "Epoch 1801/3000 - Loss: 0.0060543460490128705\n",
      "Epoch 1901/3000 - Loss: 0.005899707870444157\n",
      "Epoch 2001/3000 - Loss: 0.005753530712159239\n",
      "Epoch 2101/3000 - Loss: 0.005697443588357876\n",
      "Epoch 2201/3000 - Loss: 0.0055343901369536335\n",
      "Epoch 2301/3000 - Loss: 0.005441361805587419\n",
      "Epoch 2401/3000 - Loss: 0.0053562819381656745\n",
      "Epoch 2501/3000 - Loss: 0.005279501976629629\n",
      "Epoch 2601/3000 - Loss: 0.005210857311041943\n",
      "Epoch 2701/3000 - Loss: 0.005134492014718503\n",
      "Epoch 2801/3000 - Loss: 0.005095457050490537\n",
      "Epoch 2901/3000 - Loss: 0.005062557034880163\n"
     ]
    }
   ],
   "source": [
    "layer_sizes = [3,30,3]\n",
    "\n",
    "ffnn = FFNN(layer_sizes = layer_sizes, \n",
    "                activation_fn = ReLU, \n",
    "                activation_fn_deriv = deriv_ReLU, \n",
    "                output_activation_fn = Linear, \n",
    "                output_activation_fn_deriv = deriv_Linear, \n",
    "                loss_fn = L2, \n",
    "                loss_fn_deriv = deriv_L2,\n",
    "                task_type='regression')\n",
    "\n",
    "ffnn.train(X.T, Y, learning_rate=0.001, epochs = 3000, batch_size=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "ab843a8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "rgb = X[100:130]\n",
    "\n",
    "hsv,_,_ = ffnn.forward(rgb.T)\n",
    "plot_colors(rgb, hsv.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "b7869c94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3000 - Loss: 0.08842824445194719\n",
      "Epoch 101/3000 - Loss: 0.08842798491145788\n",
      "Epoch 201/3000 - Loss: 0.08842798491145788\n",
      "Epoch 301/3000 - Loss: 0.08842798491145788\n",
      "Epoch 401/3000 - Loss: 0.08842798491145788\n",
      "Epoch 501/3000 - Loss: 0.08842798491145788\n",
      "Epoch 601/3000 - Loss: 0.08842798491145788\n",
      "Epoch 701/3000 - Loss: 0.08842798491145788\n",
      "Epoch 801/3000 - Loss: 0.08842798491145788\n",
      "Epoch 901/3000 - Loss: 0.08842798491145788\n",
      "Epoch 1001/3000 - Loss: 0.08842798491145788\n",
      "Epoch 1101/3000 - Loss: 0.08842798491145788\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/h4/77l8l2bd37z5_jkwbw_75nsw0000gn/T/ipykernel_6955/3709372900.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mffnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.004\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/h4/77l8l2bd37z5_jkwbw_75nsw0000gn/T/ipykernel_6955/642475414.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, X, labels, learning_rate, epochs, batch_size)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m                     \u001b[0;31m# Backward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m                     \u001b[0mdL_dw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdL_db\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m                     \u001b[0;31m# Update weights and biases\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/h4/77l8l2bd37z5_jkwbw_75nsw0000gn/T/ipykernel_6955/642475414.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, x, t, a, z)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0mdL_da\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdL_dz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0mdL_dz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdL_da\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation_fn_deriv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m             \u001b[0mdL_dw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdL_dz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0mdL_db\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdL_dz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/h4/77l8l2bd37z5_jkwbw_75nsw0000gn/T/ipykernel_6955/503705229.py\u001b[0m in \u001b[0;36mderiv_ReLU\u001b[0;34m(Z)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaximum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mderiv_ReLU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mZ\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ffnn.train(X.T, Y, learning_rate=0.004, epochs = 3000, batch_size=2000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "345.352px",
    "left": "1410.98px",
    "right": "20px",
    "top": "69.9707px",
    "width": "379.482px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
